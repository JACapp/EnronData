{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enron Data POI Classifier \n",
    "### Jo Anna Capp\n",
    "\n",
    "In the 1990's Enron Corporation was one of the largest.....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set working directory\n",
    "import os\n",
    "os.chdir('D:/Documents/Udacity/IntroMachineLearning/ud120projectsmaster/ud120projectsmaster/UdacityP5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import all packages and modules here\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "import pandas\n",
    "import numpy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from ggplot import *\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_list = ['poi']\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the structure of the dataset and check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  146 executives of interest in the Enron dataset\n",
      "There are  18 identified persons of interest within the dataset\n",
      "Data Dictionary Keys:\n",
      "['METTS MARK', 'BAXTER JOHN C', 'ELLIOTT STEVEN', 'CORDES WILLIAM R', 'HANNON KEVIN P', 'MORDAUNT KRISTINA M', 'MEYER ROCKFORD G', 'MCMAHON JEFFREY', 'HORTON STANLEY C', 'PIPER GREGORY F', 'HUMPHREY GENE E', 'UMANOFF ADAM S', 'BLACHMAN JEREMY M', 'SUNDE MARTIN', 'GIBBS DANA R', 'LOWRY CHARLES P', 'COLWELL WESLEY', 'MULLER MARK S', 'JACKSON CHARLENE R', 'WESTFAHL RICHARD K', 'WALTERS GARETH W', 'WALLS JR ROBERT H', 'KITCHEN LOUISE', 'CHAN RONNIE', 'BELFER ROBERT', 'SHANKMAN JEFFREY A', 'WODRASKA JOHN', 'BERGSIEKER RICHARD P', 'URQUHART JOHN A', 'BIBI PHILIPPE A', 'RIEKER PAULA H', 'WHALEY DAVID A', 'BECK SALLY W', 'HAUG DAVID L', 'ECHOLS JOHN B', 'MENDELSOHN JOHN', 'HICKERSON GARY J', 'CLINE KENNETH W', 'LEWIS RICHARD', 'HAYES ROBERT E', 'MCCARTY DANNY J', 'KOPPER MICHAEL J', 'LEFF DANIEL P', 'LAVORATO JOHN J', 'BERBERIAN DAVID', 'DETMERING TIMOTHY J', 'WAKEHAM JOHN', 'POWERS WILLIAM', 'GOLD JOSEPH', 'BANNANTINE JAMES M', 'DUNCAN JOHN H', 'SHAPIRO RICHARD S', 'SHERRIFF JOHN R', 'SHELBY REX', 'LEMAISTRE CHARLES', 'DEFFNER JOSEPH M', 'KISHKILL JOSEPH G', 'WHALLEY LAWRENCE G', 'MCCONNELL MICHAEL S', 'PIRO JIM', 'DELAINEY DAVID W', 'SULLIVAN-SHAKLOVITZ COLLEEN', 'WROBEL BRUCE', 'LINDHOLM TOD A', 'MEYER JEROME J', 'LAY KENNETH L', 'BUTTS ROBERT H', 'OLSON CINDY K', 'MCDONALD REBECCA', 'CUMBERLAND MICHAEL S', 'GAHN ROBERT S', 'MCCLELLAN GEORGE', 'HERMANN ROBERT J', 'SCRIMSHAW MATTHEW', 'GATHMANN WILLIAM D', 'HAEDICKE MARK E', 'BOWEN JR RAYMOND M', 'GILLIS JOHN', 'FITZGERALD JAY L', 'MORAN MICHAEL P', 'REDMOND BRIAN L', 'BAZELIDES PHILIP J', 'BELDEN TIMOTHY N', 'DURAN WILLIAM D', 'THORN TERENCE H', 'FASTOW ANDREW S', 'FOY JOE', 'CALGER CHRISTOPHER F', 'RICE KENNETH D', 'KAMINSKI WINCENTY J', 'LOCKHART EUGENE E', 'COX DAVID', 'OVERDYKE JR JERE C', 'PEREIRA PAULO V. FERRAZ', 'STABLER FRANK', 'SKILLING JEFFREY K', 'BLAKE JR. NORMAN P', 'SHERRICK JEFFREY B', 'PRENTICE JAMES', 'GRAY RODNEY', 'PICKERING MARK R', 'THE TRAVEL AGENCY IN THE PARK', 'NOLES JAMES L', 'KEAN STEVEN J', 'TOTAL', 'FOWLER PEGGY', 'WASAFF GEORGE', 'WHITE JR THOMAS E', 'CHRISTODOULOU DIOMEDES', 'ALLEN PHILLIP K', 'SHARP VICTORIA T', 'JAEDICKE ROBERT', 'WINOKUR JR. HERBERT S', 'BROWN MICHAEL', 'BADUM JAMES P', 'HUGHES JAMES A', 'REYNOLDS LAWRENCE', 'DIMICHELE RICHARD G', 'BHATNAGAR SANJAY', 'CARTER REBECCA C', 'BUCHANAN HAROLD G', 'YEAP SOON', 'MURRAY JULIA H', 'GARLAND C KEVIN', 'DODSON KEITH', 'YEAGER F SCOTT', 'HIRKO JOSEPH', 'DIETRICH JANET R', 'DERRICK JR. JAMES V', 'FREVERT MARK A', 'PAI LOU L', 'BAY FRANKLIN R', 'HAYSLETT RODERICK J', 'FUGH JOHN L', 'FALLON JAMES B', 'KOENIG MARK E', 'SAVAGE FRANK', 'IZZO LAWRENCE L', 'TILNEY ELIZABETH A', 'MARTIN AMANDA K', 'BUY RICHARD B', 'GRAMM WENDY L', 'CAUSEY RICHARD A', 'TAYLOR MITCHELL S', 'DONAHUE JR JEFFREY M', 'GLISAN JR BEN F']\n",
      "A typical key:value list:  {'salary': 1111258, 'to_messages': 3627, 'deferral_payments': 'NaN', 'total_payments': 8682716, 'exercised_stock_options': 19250000, 'bonus': 5600000, 'restricted_stock': 6843672, 'shared_receipt_with_poi': 2042, 'restricted_stock_deferred': 'NaN', 'total_stock_value': 26093672, 'expenses': 29336, 'loan_advances': 'NaN', 'from_messages': 108, 'other': 22122, 'from_this_person_to_poi': 30, 'poi': True, 'director_fees': 'NaN', 'deferred_income': 'NaN', 'long_term_incentive': 1920000, 'email_address': 'jeff.skilling@enron.com', 'from_poi_to_this_person': 88}\n"
     ]
    }
   ],
   "source": [
    "#total individuals\n",
    "print \"There are \", len(data_dict.keys()), \"executives of interest in the Enron dataset\"\n",
    "#number of pois\n",
    "num_poi = 0\n",
    "for dic in data_dict.values():\n",
    "    if dic['poi'] == 1: \n",
    "        num_poi += 1\n",
    "print \"There are \", num_poi, \"identified persons of interest within the dataset\"\n",
    "print \"Data Dictionary Keys:\"\n",
    "print(data_dict.keys())\n",
    "#data dictionary format\n",
    "print \"A typical key:value list: \", data_dict[\"SKILLING JEFFREY K\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can see from this brief exploration that there are 146 exectives in the dataset, 18 identified POIs, and 21 features, for a total of 3066 observations. There are also a number of missing values. I'll investigate those in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA and Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values for each feature:\n",
      "bonus                         64\n",
      "deferral_payments            107\n",
      "deferred_income               97\n",
      "director_fees                129\n",
      "exercised_stock_options       44\n",
      "expenses                      51\n",
      "from_messages                 60\n",
      "from_poi_to_this_person       60\n",
      "from_this_person_to_poi       60\n",
      "loan_advances                142\n",
      "long_term_incentive           80\n",
      "other                         53\n",
      "poi                            0\n",
      "restricted_stock              36\n",
      "restricted_stock_deferred    128\n",
      "salary                        51\n",
      "shared_receipt_with_poi       60\n",
      "to_messages                   60\n",
      "total_payments                21\n",
      "total_stock_value             20\n",
      "dtype: int64\n",
      "Shape of the dataframe:  (146, 20)\n"
     ]
    }
   ],
   "source": [
    "#change dataset to pandas dataframe\n",
    "df = pandas.DataFrame.from_records(list(data_dict.values()))\n",
    "#delete 'email addresses' from df\n",
    "if 'email_address' in list(df.columns.values):\n",
    "    df.drop('email_address', axis=1, inplace=True)\n",
    "\n",
    "persons = pandas.Series(list(data_dict.keys()))\n",
    "\n",
    "#count number of NA values\n",
    "df.replace(to_replace='NaN', value=numpy.nan, inplace=True)\n",
    "print \"Number of NaN values for each feature:\"\n",
    "print df.isnull().sum()\n",
    "print \"Shape of the dataframe: \", df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a lot of NaN values for some of the features. Particularly loan advances, director fees, restricted stock deferred, and deferral payments. I will not add these features to my classifier. For the rest of the features, I may try to impute the missing data, or I may discard the feature. I'll look at the structure of each feature and correllations between features next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bonus</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>expenses</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>other</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>salary</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>total_stock_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.200000e+01</td>\n",
       "      <td>4.900000e+01</td>\n",
       "      <td>1.020000e+02</td>\n",
       "      <td>9.500000e+01</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>6.600000e+01</td>\n",
       "      <td>9.300000e+01</td>\n",
       "      <td>1.100000e+02</td>\n",
       "      <td>9.500000e+01</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>1.250000e+02</td>\n",
       "      <td>1.260000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.374235e+06</td>\n",
       "      <td>-1.140475e+06</td>\n",
       "      <td>5.987054e+06</td>\n",
       "      <td>1.087289e+05</td>\n",
       "      <td>608.790698</td>\n",
       "      <td>64.895349</td>\n",
       "      <td>41.232558</td>\n",
       "      <td>1.470361e+06</td>\n",
       "      <td>9.190650e+05</td>\n",
       "      <td>2.321741e+06</td>\n",
       "      <td>5.621943e+05</td>\n",
       "      <td>1176.465116</td>\n",
       "      <td>2073.860465</td>\n",
       "      <td>5.081526e+06</td>\n",
       "      <td>6.773957e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.071333e+07</td>\n",
       "      <td>4.025406e+06</td>\n",
       "      <td>3.106201e+07</td>\n",
       "      <td>5.335348e+05</td>\n",
       "      <td>1841.033949</td>\n",
       "      <td>86.979244</td>\n",
       "      <td>100.073111</td>\n",
       "      <td>5.942759e+06</td>\n",
       "      <td>4.589253e+06</td>\n",
       "      <td>1.251828e+07</td>\n",
       "      <td>2.716369e+06</td>\n",
       "      <td>1178.317641</td>\n",
       "      <td>2582.700981</td>\n",
       "      <td>2.906172e+07</td>\n",
       "      <td>3.895777e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.000000e+04</td>\n",
       "      <td>-2.799289e+07</td>\n",
       "      <td>3.285000e+03</td>\n",
       "      <td>1.480000e+02</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.922300e+04</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>-2.604490e+06</td>\n",
       "      <td>4.770000e+02</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>1.480000e+02</td>\n",
       "      <td>-4.409300e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.312500e+05</td>\n",
       "      <td>-6.948620e+05</td>\n",
       "      <td>5.278862e+05</td>\n",
       "      <td>2.261400e+04</td>\n",
       "      <td>22.750000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.812500e+05</td>\n",
       "      <td>1.215000e+03</td>\n",
       "      <td>2.540180e+05</td>\n",
       "      <td>2.118160e+05</td>\n",
       "      <td>249.750000</td>\n",
       "      <td>541.250000</td>\n",
       "      <td>3.944750e+05</td>\n",
       "      <td>4.945102e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.693750e+05</td>\n",
       "      <td>-1.597920e+05</td>\n",
       "      <td>1.310814e+06</td>\n",
       "      <td>4.695000e+04</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.420350e+05</td>\n",
       "      <td>5.238200e+04</td>\n",
       "      <td>4.517400e+05</td>\n",
       "      <td>2.599960e+05</td>\n",
       "      <td>740.500000</td>\n",
       "      <td>1211.000000</td>\n",
       "      <td>1.101393e+06</td>\n",
       "      <td>1.102872e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>-3.834600e+04</td>\n",
       "      <td>2.547724e+06</td>\n",
       "      <td>7.995250e+04</td>\n",
       "      <td>145.500000</td>\n",
       "      <td>72.250000</td>\n",
       "      <td>24.750000</td>\n",
       "      <td>9.386720e+05</td>\n",
       "      <td>3.620960e+05</td>\n",
       "      <td>1.002370e+06</td>\n",
       "      <td>3.121170e+05</td>\n",
       "      <td>1888.250000</td>\n",
       "      <td>2634.750000</td>\n",
       "      <td>2.093263e+06</td>\n",
       "      <td>2.949847e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.734362e+07</td>\n",
       "      <td>-8.330000e+02</td>\n",
       "      <td>3.117640e+08</td>\n",
       "      <td>5.235198e+06</td>\n",
       "      <td>14368.000000</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>609.000000</td>\n",
       "      <td>4.852193e+07</td>\n",
       "      <td>4.266759e+07</td>\n",
       "      <td>1.303223e+08</td>\n",
       "      <td>2.670423e+07</td>\n",
       "      <td>5521.000000</td>\n",
       "      <td>15149.000000</td>\n",
       "      <td>3.098866e+08</td>\n",
       "      <td>4.345095e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              bonus  deferred_income  exercised_stock_options      expenses  \\\n",
       "count  8.200000e+01     4.900000e+01             1.020000e+02  9.500000e+01   \n",
       "mean   2.374235e+06    -1.140475e+06             5.987054e+06  1.087289e+05   \n",
       "std    1.071333e+07     4.025406e+06             3.106201e+07  5.335348e+05   \n",
       "min    7.000000e+04    -2.799289e+07             3.285000e+03  1.480000e+02   \n",
       "25%    4.312500e+05    -6.948620e+05             5.278862e+05  2.261400e+04   \n",
       "50%    7.693750e+05    -1.597920e+05             1.310814e+06  4.695000e+04   \n",
       "75%    1.200000e+06    -3.834600e+04             2.547724e+06  7.995250e+04   \n",
       "max    9.734362e+07    -8.330000e+02             3.117640e+08  5.235198e+06   \n",
       "\n",
       "       from_messages  from_poi_to_this_person  from_this_person_to_poi  \\\n",
       "count      86.000000                86.000000                86.000000   \n",
       "mean      608.790698                64.895349                41.232558   \n",
       "std      1841.033949                86.979244               100.073111   \n",
       "min        12.000000                 0.000000                 0.000000   \n",
       "25%        22.750000                10.000000                 1.000000   \n",
       "50%        41.000000                35.000000                 8.000000   \n",
       "75%       145.500000                72.250000                24.750000   \n",
       "max     14368.000000               528.000000               609.000000   \n",
       "\n",
       "       long_term_incentive         other  restricted_stock        salary  \\\n",
       "count         6.600000e+01  9.300000e+01      1.100000e+02  9.500000e+01   \n",
       "mean          1.470361e+06  9.190650e+05      2.321741e+06  5.621943e+05   \n",
       "std           5.942759e+06  4.589253e+06      1.251828e+07  2.716369e+06   \n",
       "min           6.922300e+04  2.000000e+00     -2.604490e+06  4.770000e+02   \n",
       "25%           2.812500e+05  1.215000e+03      2.540180e+05  2.118160e+05   \n",
       "50%           4.420350e+05  5.238200e+04      4.517400e+05  2.599960e+05   \n",
       "75%           9.386720e+05  3.620960e+05      1.002370e+06  3.121170e+05   \n",
       "max           4.852193e+07  4.266759e+07      1.303223e+08  2.670423e+07   \n",
       "\n",
       "       shared_receipt_with_poi   to_messages  total_payments  \\\n",
       "count                86.000000     86.000000    1.250000e+02   \n",
       "mean               1176.465116   2073.860465    5.081526e+06   \n",
       "std                1178.317641   2582.700981    2.906172e+07   \n",
       "min                   2.000000     57.000000    1.480000e+02   \n",
       "25%                 249.750000    541.250000    3.944750e+05   \n",
       "50%                 740.500000   1211.000000    1.101393e+06   \n",
       "75%                1888.250000   2634.750000    2.093263e+06   \n",
       "max                5521.000000  15149.000000    3.098866e+08   \n",
       "\n",
       "       total_stock_value  \n",
       "count       1.260000e+02  \n",
       "mean        6.773957e+06  \n",
       "std         3.895777e+07  \n",
       "min        -4.409300e+04  \n",
       "25%         4.945102e+05  \n",
       "50%         1.102872e+06  \n",
       "75%         2.949847e+06  \n",
       "max         4.345095e+08  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#delete features with large number of missing values\n",
    "for column, series in df.iteritems():\n",
    "    if series.isnull().sum() > 100:\n",
    "        df.drop(column, axis=1, inplace=True)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pairplot to visualize feature distributions\n",
    "def splom_viz(df, labels=None):\n",
    "    ax = sns.pairplot(df, hue=\"poi\", diag_kind='kde', size=2, vars=['poi','salary', 'total_payments', 'bonus', \n",
    "                 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', \n",
    "                 'restricted_stock', 'to_messages', 'from_poi_to_this_person', 'from_messages',\n",
    "                 'from_this_person_to_poi', 'shared_receipt_with_poi'])\n",
    "    plt.show()\n",
    "\n",
    "splom_viz(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this data, there definitely appears to be outliers. If I look at the data dictioary keys again, I see that there are two that are not names: Total and travel agency in the park. I'll remove these then look again at the pairplot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#outlier removal\n",
    "df_subset = df.drop(df.index[[data_dict.keys().index(\"TOTAL\"), data_dict.keys().index(\"THE TRAVEL AGENCY IN THE PARK\")]])\n",
    "df_subset.describe()\n",
    "\n",
    "#pairplot to visualize distributions and correllations\n",
    "splom_viz(df_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After these outliers are removed, I can see on the pairplot that the most of the remainder of the outliers are classified as POI, so these \"outliers\" are in fact real data. The exception to this are the features 'from_poi_to_this_person', 'from_this_person_to_poi', and 'from_messages'. Looking at the statistics above, we see that the max 'from_messages' is 14368, which is one order of magnitude higher than the 75%. The same is true for the outliers in the other two categories.  Who are these people?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max from_poi_to_this_person:  LAVORATO JOHN J\n",
      "Max from_this_person_to_poi:  DELAINEY DAVID W\n",
      "Max from_messages:  KAMINSKI WINCENTY J\n"
     ]
    }
   ],
   "source": [
    "#identify keys for potential outliers\n",
    "for key, value in data_dict.items():\n",
    "    if value['from_poi_to_this_person'] != 'NaN' and value['from_poi_to_this_person'] > 500: \n",
    "        print \"Max from_poi_to_this_person: \", key\n",
    "\n",
    "for key, value in data_dict.items():\n",
    "    if value['from_this_person_to_poi'] != 'NaN' and value['from_this_person_to_poi'] > 500: \n",
    "        print \"Max from_this_person_to_poi: \", key\n",
    "        \n",
    "for key, value in data_dict.items():\n",
    "    if value['from_messages'] != 'NaN' and value['from_messages'] > 14000: \n",
    "        print \"Max from_messages: \", key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these keys are all different people, I will keep the email data and assume it is real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
